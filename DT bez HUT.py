import pandas as pd
from datetime import datetime
import re
import numpy as np
import warnings
import matplotlib.pyplot as plt
from collections import Counter
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
df_transformed = pd.read_csv("data_app")
df = df_transformed
df_full = df_transformed.copy()
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import shap
import matplotlib.pyplot as plt

# --- Preprocessing ---
if 'Pohlavie' in df.columns:
    df['Pohlavie'] = df['Pohlavie'].map({'M': 0, 'F': 1})

blood_pressure_cols = ['A2', 'A4', 'A6', 'A8']
pulse_cols = ['A3', 'A5', 'A7', 'A9']

for col in blood_pressure_cols:
    if col in df.columns:
        df[[f'{col}_systolic', f'{col}_diastolic']] = df[col].astype(str).str.extract(r'(\d+)/(\d+)').astype(float)
        df.drop(columns=[col], inplace=True)

for col in pulse_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

block_A_cols = [
    'A1', 'A9',
    'A2_systolic', 'A2_diastolic', 'A3',
    'A4_systolic', 'A4_diastolic', 'A5',
    'A6_systolic', 'A6_diastolic', 'A7',
    'A8_systolic', 'A8_diastolic'
]
df.drop(columns=[col for col in block_A_cols if col in df.columns], inplace=True)

# --- Pr√≠prava datasetu ---
df = df.select_dtypes(include=[float, int])
df_clean = df[df["Synkopa"].isin([0, 1])].copy()
df_clean["Synkopa"] = df_clean["Synkopa"].astype(int)

X = df_clean.drop(columns=["Synkopa", "Typ Synkopy"], errors='ignore').copy()
y = df_clean["Synkopa"]
X = X.fillna(-1).astype(float)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# --- Decision Tree s GridSearch ---
param_grid_dt = {
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

grid_dt = GridSearchCV(
    estimator=DecisionTreeClassifier(random_state=42, class_weight='balanced'),
    param_grid=param_grid_dt,
    cv=StratifiedKFold(3),
    scoring='f1_weighted',
    n_jobs=-1
)
grid_dt.fit(X_train, y_train)
best_dt = grid_dt.best_estimator_
from sklearn.tree import export_text, plot_tree
import matplotlib.pyplot as plt

# --- –°–ø–∏—Å–æ–∫ –±–∏–Ω–∞—Ä–Ω—ã—Ö –∏ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---
binary_cols = [col for col in X.columns if set(X[col].dropna().unique()).issubset({0, 1, -1})]
numeric_cols = [col for col in X.columns if col not in binary_cols]

# --- –§—É–Ω–∫—Ü–∏—è –ª–æ–≥–∏—á–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª ---
def logical_export_text(decision_tree, feature_names, binary_cols):
    rules_text = export_text(decision_tree, feature_names=feature_names)
    lines = rules_text.split('\n')
    new_lines = []

    for line in lines:
        if "<=" in line or ">" in line:
            for col in binary_cols:
                if f"{col} <= 0.50" in line or f"{col} <= 0.5" in line:
                    line = line.replace(f"{col} <= 0.50", f"{col} = 0").replace(f"{col} <= 0.5", f"{col} = 0")
                elif f"{col} >  0.50" in line or f"{col} >  0.5" in line:
                    line = line.replace(f"{col} >  0.50", f"{col} = 1").replace(f"{col} >  0.5", f"{col} = 1")
        new_lines.append(line)
    return '\n'.join(new_lines)

# --- –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ ---
y_pred = best_dt.predict(X_test)
y_proba = best_dt.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
f1 = f1_score(y_test, y_pred, average='weighted')
roc_auc = roc_auc_score(y_test, y_proba)

print(f"Decision Tree - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}, AUC: {roc_auc:.3f}")

# --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ä–µ–≤–∞ ---
plt.figure(figsize=(50, 40))
plot_tree(
    best_dt,
    feature_names=X.columns,
    class_names=["Negat√≠vny", "Pozit√≠vny"],
    filled=True,
    rounded=True,
    fontsize=10,
    max_depth=None
)
plt.title("Vizualiz√°cia rozhodovacieho stromu pre predikciu synkopy (HUT test)")
plt.tight_layout()
plt.savefig("strom.png", dpi=300)

from sklearn.tree import _tree

def get_rules(tree, feature_names, binary_cols, target_names):
    tree_ = tree.tree_
    feature_name = [
        feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
        for i in tree_.feature
    ]

    paths = []
    path = []

    def recurse(node, path, paths):
        if tree_.feature[node] != _tree.TREE_UNDEFINED:
            name = feature_name[node]
            threshold = tree_.threshold[node]

            if name in binary_cols:
                # –î–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (0 –∏–ª–∏ 1)
                recurse(tree_.children_left[node],
                        path + [f"({name} = 0)"], paths)
                recurse(tree_.children_right[node],
                        path + [f"({name} = 1)"], paths)
            else:
                # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                recurse(tree_.children_left[node],
                        path + [f"({name} <= {threshold:.2f})"], paths)
                recurse(tree_.children_right[node],
                        path + [f"({name} > {threshold:.2f})"], paths)
        else:
            # –õ–∏—Å—Ç –¥–µ—Ä–µ–≤–∞ (—Ä–µ—à–µ–Ω–∏–µ)
            value = tree_.value[node][0]
            class_idx = value.argmax()
            probability = value[class_idx] / value.sum()
            path_statement = " AND ".join(path)
            rule = f"IF {path_statement} THEN class = {target_names[class_idx]} (probability = {probability:.2f})"
            paths.append(rule)

    recurse(0, path, paths)

    return paths

# –°–ø–∏—Å–æ–∫ –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ç–æ–ª—å–∫–æ 0/1)
binary_cols = [col for col in X.columns if set(X[col].dropna().unique()).issubset({0, 1, -1})]

# –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—á–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª:
rules = get_rules(best_dt, feature_names=X.columns, binary_cols=binary_cols, target_names=["Negat√≠vny", "Pozit√≠vny"])

# –ü–µ—á–∞—Ç—å –ø–µ—Ä–≤—ã—Ö 20 –ø—Ä–∞–≤–∏–ª (–¥–ª—è –ø—Ä–∏–º–µ—Ä–∞)
for i, rule in enumerate(rules[:100], start=1):
    print(f"R{i}: {rule}")
# --- Post-pruning pomocou cost-complexity pruning ---
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—É—Ç–∏ –æ–±—Ä–µ–∑–∫–∏
path = best_dt.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

# –°—Ç—Ä–æ–∏–º –¥–µ—Ä–µ–≤—å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è ccp_alpha
dt_models = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(
        random_state=42,
        class_weight='balanced',
        ccp_alpha=ccp_alpha
    )
    clf.fit(X_train, y_train)
    dt_models.append(clf)

# –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤—Å–µ—Ö –¥–µ—Ä–µ–≤—å–µ–≤
from sklearn.metrics import f1_score

f1_scores = [f1_score(y_test, clf.predict(X_test), average='weighted') for clf in dt_models]

# –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–ª—É—á—à–µ–µ –¥–µ—Ä–µ–≤–æ –ø–æ F1
best_idx = np.argmax(f1_scores)
best_pruned_dt = dt_models[best_idx]

print(f"üéØ Best ccp_alpha: {ccp_alphas[best_idx]:.5f}, F1 Score: {f1_scores[best_idx]:.3f}")

# –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –¥–µ—Ä–µ–≤–∞ –≤ PNG
plt.figure(figsize=(50, 40))
plot_tree(
    best_pruned_dt,
    feature_names=X.columns,
    class_names=["Negat√≠vny", "Pozit√≠vny"],
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title("Orezan√© rozhodovacie strom (post-pruning s CCP)")
plt.tight_layout()
plt.savefig("strom.png", dpi=300)


# --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –∏–∑ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞ ---
rules_pruned = get_rules(
    best_pruned_dt,
    feature_names=X.columns,
    binary_cols=binary_cols,
    target_names=["Negat√≠vny", "Pozit√≠vny"]
)

# –ü–µ—á–∞—Ç—å –ø–µ—Ä–≤—ã—Ö 100 –ø—Ä–∞–≤–∏–ª –ø–æ—Å–ª–µ pruning
for i, rule in enumerate(rules_pruned[:100], start=1):
    print(f"PRUNED R{i}: {rule}")
# from sklearn.linear_model import LogisticRegression
# # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å Logistic Regression
# log_reg = LogisticRegression(max_iter=1000, random_state=42)
# log_reg.fit(X_train, y_train)
# # --- –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ ---
# y_pred = log_reg.predict(X_test)
# y_proba = log_reg.predict_proba(X_test)[:, 1]
#
# accuracy = accuracy_score(y_test, y_pred)
# precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
# recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
# f1 = f1_score(y_test, y_pred, average='weighted')
# roc_auc = roc_auc_score(y_test, y_proba)
#
# print(f"Log Reg - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}, AUC: {roc_auc:.3f}")
# # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã (–≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
# coefficients = log_reg.coef_[0]  # –ü–æ–ª—É—á–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# selected_features = X.columns  # –ü—Ä–∏–∑–Ω–∞–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
#
# # –°–æ–∑–¥–∞—ë–º DataFrame –¥–ª—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# df_importance = pd.DataFrame({
#     "Feature": selected_features,
#     "Importance": coefficients
# }).sort_values("Importance", ascending=False)
#
# # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# plt.figure(figsize=(40, 40))
# plt.barh(df_importance["Feature"], df_importance["Importance"], color='purple')
# plt.xlabel("Importances")
# plt.title("Importance Logistic Regression")
# plt.gca().invert_yaxis()  # –ß—Ç–æ–±—ã —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±—ã–ª–∏ —Å–≤–µ—Ä—Ö—É
# plt.grid(axis='x', linestyle='--', alpha=0.5)
# plt.tight_layout()
# plt.show()
import joblib
joblib.dump(best_dt, "best_decision_tree_model.pkl")
